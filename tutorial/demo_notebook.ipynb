{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MRPA-LegNet**: Predicting gene expression from human massively parallel reporter assays with SOTA convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial presents the use of MRPA-LegNet, a variant of LegNet ([Paper](https://doi.org/10.1093/bioinformatics/btad457),\n",
    "[Repo](https://github.com/autosome-ru/LegNet/)) that was specifically modified and optimized for predicting gene expression from human massive parallel reporter assays performed with human K562, HepG2, and WTC11 cell lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't hesitate to ask questions or share any feedback: dmitrypenzar1996@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, a set of sequences with corresponding expression values is needed. We use MPRA data from the accompanying ZENODO repository [*link*] for the purposes of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The formal description of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the standard LegNet version ([Paper](https://doi.org/10.1093/bioinformatics/btad457),\n",
    "[Repo](https://github.com/autosome-ru/LegNet/)), this version solves the problem of predicting gene expression from human massive parallel reporter assays in the classical setting, i.e. regression to single expression value, see Fig. 2b in [de Boer et al. (2020)](https://doi.org/10.1038/s41587-019-0315-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tab-separated training data must use the following format:\n",
    "\n",
    "- First column: sequence. Second column: expression value. Third column: fold (optional).\n",
    "- The data should be provided without any extra header line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***model_dir*** - url of folder for all outputs;\n",
    "* ***data_path*** - url of folder for input data;\n",
    "* ***delimeter*** - delimeter between columns in training data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('model')\n",
    "data_path = Path('dataset')\n",
    "delimiter = \"\\t\"\n",
    "\n",
    "train_path = data_path / 'WTC11_train.tsv'\n",
    "valid_path = data_path / 'WTC11_valid.tsv'\n",
    "test_path = data_path / 'WTC11_test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are required to have equal lengths. We'll use both shift and reverse sequences augmentation. Shift augmentation requires a pre-defined surrounding plasmid sequence (forward and reverse sides). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify length of sequences (*seqsize*) and maximum shift in both directions for data augmentation (*max_shift*, first element determines shift to the forward side, second - to the reverse side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_side = \"GGCCCGCTCTAGACCTGCAGG\"\n",
    "reverse_side = \"CACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGT\"\n",
    "seqsize = 230 \n",
    "max_shift = (len(forward_side),\n",
    "             len(reverse_side))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***in_ch*** - number of input channels for the model (one for each nucleotide, which adds up to 4, one additional for information on whether the original sequence was subjected to the reverse complementary transformation);\n",
    "* ***stem_ch*** - number of output channels of the first (stem) block;\n",
    "* ***stem_ks*** - kernel size of one-dimensional convolutional layer in the stem block;\n",
    "* ***ef_ks*** - kernel size of convolutional layers in EfficientNet-like bloks;\n",
    "* ***ef_block_sizes*** - list containing the number of channels for each EfficientNet-like block. The number of elements must be the same as in the next parameter;\n",
    "* ***pool_sizes*** - list containing the parameters for 1D max pooling of each EfficientNet-like block. The number of elements must be the same as in the previous parameter;\n",
    "* ***resize_factor*** - inverse of reduction parameter in squeeze-and-excitation blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 5\n",
    "stem_ch = 64\n",
    "stem_ks = 11\n",
    "ef_ks = 9\n",
    "ef_block_sizes = [80, 96, 112, 128]\n",
    "pool_sizes = [2, 2, 2, 2]\n",
    "resize_factor = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation arguments\n",
    "* ***reverse_augment*** - whether or not augment data with reverse complementary sequences;\n",
    "* ***use_reverse_channel*** - whether or not add reverse augmentation channel. **Note, that this is not the same as reverse_augment**. The channel is usually supplied with either zeros (denoting forward) or ones (reverse complementary sequence);\n",
    "* ***use_shift*** - whether or not augment data with shifted sequences (maximum possible degree of shift is defined in `max_shift` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_augment = True\n",
    "use_reverse_channel = True\n",
    "use_shift = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer & Scheduler arguments\n",
    "* ***max_lr*** - maximum learing rate during One Cycle Policy (OneCycleLR);\n",
    "* ***weight_decay*** - weight decay coefficient for AdamW alghoritm;\n",
    "* ***epoch_num*** - total number of epochs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.1\n",
    "weight_decay = 0.1\n",
    "epoch_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "train_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "weight_decay = 0.01\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=1\n",
    "seed = 777\n",
    "set_global_seed(seed)\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Input vector structure\n",
    "Training sequences were encoded into 4-dimensional vectors using one-hot encoding (`Seq2Tensor` class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the regulatory elements are often asymmetric relative to the transcription start sites, **different scores are expected for direct and reverse complementary strands of a particular sequence**. Thus, the data can be augmented by providing each sequence twice in *native* and *reverse complementary form*, specifying `0` and `1`, respectively, in an additional `is_reverse` channel. The test-time augmentation can be performed by averaging the predictions made for direct (`is_reverse=0`) and reverse complementary (`is_reverse=1`) input.\n",
    "\n",
    "![One hot encoding of input sequence](https://raw.githubusercontent.com/autosome-ru/LegNet/b0432ddead56e455b0d54ca798b5408dc496a8e7/tutorial/img/Input.jpg)\n",
    "\n",
    "**Can I remove singleton channel and reuse this image?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Tensor(nn.Module):\n",
    "    '''\n",
    "    Encode sequences using one-hot encoding after preprocessing.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, seq):\n",
    "        if isinstance(seq, torch.FloatTensor):\n",
    "            return seq\n",
    "        seq = [n2id(x) for x in seq]\n",
    "        code = torch.from_numpy(np.array(seq))\n",
    "        code = F.one_hot(code, num_classes=5) # 5th class is N\n",
    "        \n",
    "        code[code[:, 4] == 1] = 0.25 # encode Ns with .25\n",
    "        code = code[:, :4].float() \n",
    "        return code.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sequence Datasets\n",
    "Trainer code uses [pytorch-lightning](https://github.com/Lightning-AI/pytorch-lightning) module.\n",
    "\n",
    "Sequence datasets are implemented in `TrainSeqDatasetProb` and `TestSeqDatasetProb` classes. The difference between the two classes lies primarily in the **\\_\\_getitem\\_\\_** method (for selecting *i*-th sequence from the `pandas.DataFrame`). \n",
    "\n",
    "During training (`TrainSeqDatasetProb`), data can be augmented in two ways - by means of shift and reverse augmentations, where sequence shift is uniformly distributed in the boundaries defined by `maxshift`. \n",
    "\n",
    "In the second case (`TestSeqDatasetProb`) sequences can be constantly shifted and/or reversed (which is not data augmentation *per se*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `TrainSeqDatasetProb`\n",
    "**\\_\\_init\\_\\_** parameters:\n",
    "- ***ds***: `pd.DataFrame` - Training dataset;\n",
    "- ***use_shift***: `bool` - If True, additional shift augmentation is used;\n",
    "- ***use_reverse***: `bool` - If True, additional reverse augmentation is used;\n",
    "- ***use_reverse_channel***: `bool` - If True, extra channel indicating reversed sequences is added. **Note that this is not the same as *use_reverse***;\n",
    "- ***seqsize***: `int` - Constant sequence length;\n",
    "- ***maxshift***: `tuple[int, int]` - Maximum shift in both directions.\n",
    "\n",
    "**\\_\\_getitem\\_\\_** output for `i`-th sequence (row) in the training dataset (`ds`):\n",
    "- ***X***: `torch.Tensor` - One-hot encoding tensor (with reverse channel if required).\n",
    "- ***bin***: `float` - Training expression value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSeqDatasetProb(Dataset):\n",
    "    def __init__(self, \n",
    "                 ds: pd.DataFrame,\n",
    "                 use_reverse: bool,\n",
    "                 use_shift: bool,\n",
    "                 use_reverse_channel: bool,  \n",
    "                 seqsize=230,\n",
    "                 max_shift: tuple[int, int] | None = None, \n",
    "                 training=True):\n",
    "        self.training = training\n",
    "\n",
    "        self.ds = ds\n",
    "        self.totensor = Seq2Tensor() \n",
    "        self.use_reverse = use_reverse\n",
    "        self.use_shift = use_shift\n",
    "        self.use_reverse_channel = use_reverse_channel\n",
    "        self.forward_side = forward_side\n",
    "        self.reverse_side = reverse_side\n",
    "        self.seqsize = seqsize \n",
    "        if max_shift is None:\n",
    "            self.max_shift = (0, len(self.forward_side))\n",
    "        else:\n",
    "            self.max_shift = max_shift\n",
    "        \n",
    "            \n",
    "    def transform(self, x):\n",
    "        assert isinstance(x, str)\n",
    "        return self.totensor(x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        seq = self.ds.seq.values[i]\n",
    "        \n",
    "        if self.use_shift:\n",
    "            shift = torch.randint(size=(1,), low=-self.max_shift[0], high=self.max_shift[1] + 1).item()\n",
    "            if shift < 0: # use forward primer\n",
    "                seq = seq[:shift]\n",
    "                seq = self.forward_side[shift:] + seq\n",
    "            elif shift > 0:\n",
    "                seq = seq[shift:]\n",
    "                seq = seq + self.reverse_side[:shift]\n",
    "            else: # shift = 0\n",
    "                pass # nothing to do\n",
    "\n",
    "        if self.use_reverse:\n",
    "            r = torch.rand((1,)).item()\n",
    "            if  r > 0.5:\n",
    "                seq = reverse_complement(seq)\n",
    "                rev = 1.0\n",
    "            else:\n",
    "                rev = 0.0\n",
    "        else:\n",
    "            rev = 0.0\n",
    "            \n",
    "        seq = self.transform(seq)\n",
    "        to_concat = [seq]\n",
    "        \n",
    "        # add reverse augmentation channel\n",
    "        if self.use_reverse_channel:\n",
    "            rev = torch.full( (1, self.seqsize), rev, dtype=torch.float32)\n",
    "            to_concat.append(rev)\n",
    "            \n",
    "        # create final tensor\n",
    "        if len(to_concat) > 1:\n",
    "            X = torch.concat(to_concat, dim=0)\n",
    "        else:\n",
    "            X = seq\n",
    "            \n",
    "        mean = self.ds.mean_value.values[i]\n",
    "        \n",
    "        return X, mean.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `TestSeqDatasetProb`\n",
    "**\\_\\_init\\_\\_** parameters:\n",
    "- ***ds***: `pd.DataFrame` - Test dataset;\n",
    "- ***shift***: `int` - All sequences shifted by a constant value;\n",
    "- ***reverse***: `bool` - If True, all sequences are reversed;\n",
    "- ***use_reverse_channel***: `bool` - If True, extra channel indicating reversed sequences is added. **Note that this is not the same as *reverse***;\n",
    "- ***seqsize***: `int` - Constant sequence length.\n",
    "\n",
    "**\\_\\_getitem\\_\\_** output for `i`-th sequence (row) in the test dataset (`ds`):\n",
    "- ***X***: `torch.Tensor` - One-hot encoding tensor (with reverse channel if required).\n",
    "- ***bin***: `float` - Test expression value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSeqDatasetProb(Dataset):\n",
    "    def __init__(self, \n",
    "                 ds: pd.DataFrame,\n",
    "                 reverse: bool,\n",
    "                 shift: int,  \n",
    "                 use_reverse_channel: bool = True,\n",
    "                 seqsize=230):       \n",
    "        self.ds = ds\n",
    "        self.totensor = Seq2Tensor()\n",
    "        self.use_reverse_channel = use_reverse_channel \n",
    "        self.reverse = reverse\n",
    "        self.shift = shift\n",
    "        self.forward_side = forward_side\n",
    "        self.reverse_side = reverse_side\n",
    "        self.seqsize = seqsize \n",
    "\n",
    "        \n",
    "    def transform(self, x):\n",
    "        assert isinstance(x, str)\n",
    "        return self.totensor(x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        seq = self.ds.seq.values[i]\n",
    "        \n",
    "        if self.shift < 0: # use forward primer\n",
    "            seq = seq[:self.shift]\n",
    "            seq = self.forward_side[self.shift:] + seq\n",
    "        elif self.shift > 0:\n",
    "            seq = seq[self.shift:]\n",
    "            seq = seq + self.reverse_side[:self.shift]\n",
    "        else: # shift = 0\n",
    "            pass # nothing to do\n",
    "\n",
    "        \n",
    "        if self.reverse:\n",
    "            seq = reverse_complement(seq)\n",
    "            rev = 1.0\n",
    "        else:\n",
    "            rev = 0.0\n",
    "\n",
    "        seq = self.transform(seq)\n",
    "        to_concat = [seq]\n",
    "        \n",
    "        # add reverse augmentation channel\n",
    "        if self.use_reverse_channel:\n",
    "            rev = torch.full( (1, self.seqsize), rev, dtype=torch.float32)\n",
    "            to_concat.append(rev)\n",
    "            \n",
    "        # create final tensor\n",
    "        if len(to_concat) > 1:\n",
    "            X = torch.concat(to_concat, dim=0)\n",
    "        else:\n",
    "            X = seq\n",
    "            \n",
    "        mean = self.ds.mean_value.values[i]\n",
    "        \n",
    "        return X, mean.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lightning Datamodule\n",
    "`SeqDataModule` inherits `lightning.pytorch.LightningDataModule`. This class loads test and train datasets for **pytorch-lightning** module. \n",
    "Note, that in *dataset.py* file from the source code this class splits unified file into train, validation and test datasets based on values in the `fold_num` column, whilst pre-splitted files are used in this tutorial.\n",
    "\n",
    "`SeqDataModule` passes ***reverse_augment***, ***use_reverse_channel***, ***use_shift***, ***max_shift*** parameters to the `TrainSeqDatasetProb` and `TestSeqDatasetProb` classes and ***train_batch_size***, ***test_batch_size***, ***num_workers*** parameters to the `torch.utils.data.DataLoader` class.\n",
    "\n",
    "- **train_dataloader** creates training DataLoader for `trainer.fit()`;\n",
    "- **val_dataloader** creates validation DataLoader for `trainer.fit()`. Based on highest Pearson coefficient between predicted and true validation expression values best-performing model across all epochs will be selected;\n",
    "- **dls_for_predictions** creates test DataLoader (with reverse sequences, if required) for `trainer.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 train_path: str,\n",
    "                 valid_path: str,\n",
    "                 test_path: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        ds_columns = ['seq', 'mean_value']\n",
    "        self.train = pd.read_csv(train_path, sep='\\t')\n",
    "        self.valid = pd.read_csv(valid_path, sep='\\t')\n",
    "        self.test = pd.read_csv(test_path, sep='\\t')\n",
    "        self.train.columns = ds_columns\n",
    "        self.valid.columns = ds_columns\n",
    "        self.test.columns = ds_columns\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_ds =  TrainSeqDatasetProb(self.train,\n",
    "                                   use_reverse=reverse_augment,\n",
    "                                   use_reverse_channel=use_reverse_channel,\n",
    "                                   use_shift=use_shift,\n",
    "                                   max_shift=max_shift)\n",
    "        \n",
    "        return DataLoader(train_ds, \n",
    "                          batch_size=train_batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=True) \n",
    "    \n",
    "    def val_dataloader(self): # test dataset\n",
    "        valid_ds = TestSeqDatasetProb(self.valid, \n",
    "                                  use_reverse_channel=use_reverse_channel,\n",
    "                                  shift=0,\n",
    "                                  reverse=False)\n",
    "\n",
    "        return DataLoader(valid_ds, \n",
    "                          batch_size=train_batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=False)\n",
    "        \n",
    "    def dls_for_predictions(self): #\n",
    "        \n",
    "        test_ds = TestSeqDatasetProb(self.test,\n",
    "                                  use_reverse_channel=use_reverse_channel,\n",
    "                                  shift=0,\n",
    "                                  reverse=False)\n",
    "        test_dl =  DataLoader(test_ds,\n",
    "                              batch_size=test_batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=False)\n",
    "        yield \"forw_pred\", test_dl\n",
    "        if reverse_augment:\n",
    "            rev_test_ds = TestSeqDatasetProb(self.test,\n",
    "                                  use_reverse_channel=use_reverse_channel,\n",
    "                                  shift=0,\n",
    "                                  reverse=True)\n",
    "            rev_test_dl =  DataLoader(rev_test_ds,\n",
    "                              batch_size=test_batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=False)\n",
    "            yield \"rev_pred\", rev_test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "Our model is based upon a fully-convolutional neural network architecture inspired by EfficientNetV2 with selected features from DenseNet and additional custom blocks.\n",
    "\n",
    "![LegNet architecture](https://raw.githubusercontent.com/autosome-ru/LegNet/b0432ddead56e455b0d54ca798b5408dc496a8e7/tutorial/img/A.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(inp, int(inp // reduction)),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(int(inp // reduction), inp),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, resize_factor, activation, out_ch=None, se_reduction=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim = self.in_ch * self.resize_factor\n",
    "        \n",
    "        block = nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       \n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=ks,\n",
    "                            groups=self.inner_dim,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       SELayer(self.inner_dim, reduction=self.se_reduction),\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.in_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.in_ch),\n",
    "                       activation(),\n",
    "        )\n",
    "        \n",
    "        self.block = block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.out_ch,\n",
    "                            kernel_size=self.ks,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.out_ch),\n",
    "                       activation()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapperBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.Conv1d(in_channels=in_features,\n",
    "                      out_channels=out_features, \n",
    "                      kernel_size=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_ch,\n",
    "                 stem_ch,\n",
    "                 stem_ks, \n",
    "                 ef_ks,\n",
    "                 ef_block_sizes,\n",
    "                 pool_sizes,\n",
    "                 resize_factor,\n",
    "                 activation=nn.SiLU,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert len(pool_sizes) == len(ef_block_sizes)\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        self.stem = LocalBlock(in_ch=in_ch, \n",
    "                               out_ch=stem_ch,\n",
    "                               ks=stem_ks,\n",
    "                               activation=activation)\n",
    "        \n",
    "        blocks = []\n",
    "       \n",
    "        in_ch = stem_ch\n",
    "        out_ch = stem_ch\n",
    "        for pool_sz, out_ch in zip(pool_sizes, ef_block_sizes):\n",
    "            blc = nn.Sequential(\n",
    "                ResidualConcat(\n",
    "                    EffBlock(\n",
    "                        in_ch=in_ch, \n",
    "                        out_ch=in_ch,\n",
    "                        ks=ef_ks,\n",
    "                        resize_factor=resize_factor,\n",
    "                        activation=activation)\n",
    "                ),\n",
    "                LocalBlock(in_ch=in_ch * 2,\n",
    "                           out_ch=out_ch,\n",
    "                           ks=ef_ks,\n",
    "                           activation=activation),\n",
    "                nn.MaxPool1d(pool_sz) if pool_sz != 1 else nn.Identity()\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "            blocks.append(blc)\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.mapper = MapperBlock(in_features=out_ch, \n",
    "                                  out_features=out_ch * 2)\n",
    "        self.head = nn.Sequential(nn.Linear(out_ch * 2, out_ch * 2),\n",
    "                                   nn.BatchNorm1d(out_ch * 2),\n",
    "                                   activation(),\n",
    "                                   nn.Linear(out_ch * 2, 1))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.main(x)\n",
    "        x = self.mapper(x)\n",
    "        x =  F.adaptive_avg_pool1d(x, 1)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Trainer\n",
    "The trainer is `LitModel` class, which inherits `lightning.pytorch.LightningModule`.\n",
    "Here we instantiate our model, initialize it's weights and pass loss function and metric for validation (`PearsonCorrCoef`).\n",
    "\n",
    "Also we define training, validation, test and predict steps for pl.LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, max_lr):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = LegNet(in_ch=in_ch,\n",
    "                            stem_ch=stem_ch,\n",
    "                            stem_ks=stem_ks,\n",
    "                            ef_ks=ef_ks,\n",
    "                            ef_block_sizes=ef_block_sizes,  \n",
    "                            resize_factor=resize_factor,\n",
    "                            pool_sizes=pool_sizes)\n",
    "        self.model.apply(initialize_weights)\n",
    "        self.loss = loss\n",
    "        self.val_pearson = PearsonCorrCoef()\n",
    "        self.max_lr = max_lr\n",
    "        \n",
    "    def training_step(self, batch, _):\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=False, on_epoch=True,  logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.val_pearson(y_hat, y)\n",
    "        self.log(\"val_pearson\", self.val_pearson, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "        \n",
    "    def predict_step(self, batch, _):\n",
    "        if isinstance(batch, (tuple, list)):\n",
    "            x, _ = batch \n",
    "        else:\n",
    "            x = batch\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=self.max_lr  / 25,\n",
    "                                      weight_decay=weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, # type: ignore\n",
    "                                                        max_lr=self.max_lr ,\n",
    "                                                        three_phase=False, \n",
    "                                                        total_steps=self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                        pct_start=0.3,\n",
    "                                                        cycle_momentum =False)\n",
    "        \n",
    "        lr_scheduler_config = {\n",
    "            # REQUIRED: The scheduler instance\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            # The unit of the scheduler's step size, could also be 'step'.\n",
    "            # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "            # updates it after a optimizer update.\n",
    "            \"interval\": \"step\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"cycle_lr\"\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder\n",
    "Here we'll define optimum learning rate value for model training.\n",
    "\n",
    "Pass `pl.Trainer` instance to `pl.tuner.Tuner`, then run `Tuner.lr_find` on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count:  tensor(1323169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/nikgr/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated stepping batches -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikgr/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/nikgr/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc141cb9a2a4eed8726748051a97184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikgr/miniconda3/envs/legnet/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at model/model/.lr_find_5a35a765-65c3-4eae-a4ba-c4ebebec5f0d.ckpt\n",
      "Restored all states from the checkpoint at model/model/.lr_find_5a35a765-65c3-4eae-a4ba-c4ebebec5f0d.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested lr: 0.0013182567385564075\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG1CAYAAAAFuNXgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/zElEQVR4nO3de3xU1b338e9MJpncExKSkJBwEwwQ5I4FCwpyCkKLeGn1tB4vte0pjxdaObzaAq29WqrSp+ijBa0XoNSjnKKWUy1KW26iVIKgqICgQAIkhBCSyXUmM7OfP4YMBgLkMsme2fm8X695mdmzZ+a3MjH5stbaa9kMwzAEAABgEXazCwAAAAglwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUh9kFdDW/36/jx48rKSlJNpvN7HIAAEArGIah6upq5eTkyG6/eN9Mtws3x48fV15entllAACAdiguLlZubu5Fz+l24SYpKUlS4JuTnJxscjUAAKA1XC6X8vLygn/HL6bbhZumoajk5GTCDQAAEaY1U0qYUAwAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACzF1HCzbNkyDR8+PLjP04QJE/S3v/3tgudv2rRJNpvtvNu+ffu6sGoAABDOTN04Mzc3V7/5zW80cOBASdLKlSs1e/Zs7dq1SwUFBRd83v79+5ttepmRkdHptQIAgIs7XlmveWt2Kys5Vo/9+yjT6jA13MyaNavZ/YceekjLli3T9u3bLxpuMjMzlZqa2snVAQCAtjhV49H2zyqUnRJrah1hM+fG5/PpxRdfVG1trSZMmHDRc0eNGqXs7GxNnTpVGzduvOi5brdbLper2Q0AAISeq6FRkpQUa2rfifnhZs+ePUpMTJTT6dScOXP0yiuvaOjQoS2em52draefflpr167Vyy+/rPz8fE2dOlVbtmy54OsvXrxYKSkpwVteXl5nNQUAgG6tOhhuok2tw2YYhmFmAR6PR0VFRaqsrNTatWv1zDPPaPPmzRcMOOeaNWuWbDab1q1b1+Ljbrdbbrc7eN/lcikvL09VVVXN5u0AAICOWVNYrB/8+QNNzs/Qim9eGdLXdrlcSklJadXfb3P7jSTFxMQEJxSPHTtWO3bs0GOPPaannnqqVc8fP368Vq9efcHHnU6nnE5nSGoFAAAXVt3glSQlm9xzY/qw1LkMw2jW03Ipu3btUnZ2didWBAAAWqM6TObcmPruCxcu1IwZM5SXl6fq6mq9+OKL2rRpk9avXy9JWrBggY4dO6ZVq1ZJkpYuXap+/fqpoKBAHo9Hq1ev1tq1a7V27VozmwEAACS56gM9N2bPuTE13Jw4cUK33367SkpKlJKSouHDh2v9+vX60pe+JEkqKSlRUVFR8HyPx6P58+fr2LFjiouLU0FBgV577TXNnDnTrCYAAIAzwqXnxvQJxV2tLROSAABA6835406t/6hUv5xdoNsn9Avpa7fl73fYzbkBAACRqdod6LlJjmNCMQAAsICzc266+SJ+AADAGsJlET/CDQAACImmdW7ouQEAAJbAIn4AAMAyGhp98vj8kui5AQAAFtC0I7jNJiXEEG4AAECEaxqSSnQ6ZLfbTK2FcAMAADosXObbSIQbAAAQAuGy9YJEuAEAACFAzw0AALAUVz09NwAAwELCZQE/iXADAABCIFy2XpAINwAAIARcTXNu4ui5AQAAFuCi5wYAAFgJc24AAIClMOcGAABYytl1bui5AQAAFnB2WIqeGwAAYAFNE4rpuQEAABHPMAx6bgAAgHXUN/rk8xuSuFoKAABYQFOvTZTdpviYKJOrIdwAAIAO+vymmTabzeRqCDcAAKCDXGG0gJ9EuAEAAB0UXMDPaf5kYolwAwAAOiictl6QCDcAAKCDgqsTx9FzAwAALODsjuD03AAAAAuoDq5OTM8NAACwAObcAAAASzm7Izg9NwAAwAKqmXMDAACsxFUfPptmSoQbAADQQVwtBQAALIUJxQAAwFKCl4KziB8AAIh0fr+hajc9NwAAwCJqPV4ZRuBrLgUHAAARr2m+TXSUTU5HeMSK8KgCAABEpM8v4Gez2UyuJoBwAwAA2i3cFvCTCDcAAKADzq5xEx7zbSTCDQAA6IBwW+NGItwAAIAOcBFuAACAlQQX8GNYCgAAWMHZYSnCDQAAsABXPVdLAQAAC2FCMQAAsBTm3Jxj2bJlGj58uJKTk5WcnKwJEybob3/720Wfs3nzZo0ZM0axsbEaMGCAli9f3kXVAgCAcwVXKI6j50aSlJubq9/85jcqLCxUYWGhrr32Ws2ePVsfffRRi+cfOnRIM2fO1KRJk7Rr1y4tXLhQc+fO1dq1a7u4cgAAIIXnIn6mxqxZs2Y1u//QQw9p2bJl2r59uwoKCs47f/ny5erTp4+WLl0qSRoyZIgKCwu1ZMkS3XzzzV1RMgAA+Bzm3FyEz+fTiy++qNraWk2YMKHFc9555x1Nmzat2bHp06ersLBQjY2NLT7H7XbL5XI1uwEAgNDgUvAW7NmzR4mJiXI6nZozZ45eeeUVDR06tMVzS0tLlZWV1exYVlaWvF6vysvLW3zO4sWLlZKSErzl5eWFvA0AAHRHPr+hGnfTruD03ATl5+dr9+7d2r59u/7P//k/uvPOO/Xxxx9f8Pxzt1M3DKPF400WLFigqqqq4K24uDh0xQMA0I01BRspvHpuTI9ZMTExGjhwoCRp7Nix2rFjhx577DE99dRT553bq1cvlZaWNjtWVlYmh8Oh9PT0Fl/f6XTK6XSGvnAAALq5pgX8nA67Yhym95cEhU8lZxiGIbfb3eJjEyZM0IYNG5ode/PNNzV27FhFR4dPYgQAoDsIx/k2ksnhZuHChdq6dasOHz6sPXv2aNGiRdq0aZNuu+02SYEhpTvuuCN4/pw5c3TkyBHNmzdPe/fu1XPPPadnn31W8+fPN6sJAAB0W2cX8DN9IKgZU6s5ceKEbr/9dpWUlCglJUXDhw/X+vXr9aUvfUmSVFJSoqKiouD5/fv31+uvv64HHnhATz75pHJycvT4449zGTgAACYI9tzEhVfPjanh5tlnn73o4ytWrDjv2DXXXKP33nuvkyoCAACtVe0Oz56bsJtzAwAAIoOrPvwW8JMINwAAoJ2a5twkOcNrWIpwAwAA2iUcN82UCDcAAKCdXFwKDgAArOTsjuD03AAAAAuooecGAABYSTU9NwAAwEqaNs5MchJuAACABbC3FAAAsJSmcJPIsBQAAIh0fr9xdliKcAMAACJdjccb/DqROTcAACDSNV0GHhNlV2x0lMnVNEe4AQAAbRau820kwg0AAGiHGnd4rnEjEW4AAEA7NO0rFW7zbSTCDQAAaIezWy8QbgAAgAWE6wJ+EuEGAAC0Q3DODcNSAADACqoZlgIAAFbCpeAAAMBSmHMDAAAspWnODZeCAwAAS2DODQAAsBTCDQAAsJQaN3NuAACAhVQ3MOcGAABYCMNSAADAMjxev9xevyQpycmwFAAAiHBN820kFvEDAAAW0DTfJj4mSlF2m8nVnI9wAwAA2iSc59tIhBsAANBG4bz1gkS4AQAAbdQ05yYcLwOXCDcAAKCNmubcMCwFAAAs4ezqxIQbAABgAcE5N2G4xo1EuAEAAG3UFG7CcY0biXADAADaiDk3AADAUoI9N1wtBQAArKBpQnEy69wAAAAraBqWYs4NAACwBLZfAAAAlsKcGwAAYClnF/Fjzg0AAIhwhmGwQjEAALCO+kaffH5DEuEGAABYQNN8myi7TXHRUSZX0zLCDQAAaLXPTya22WwmV9Mywg0AAGi1cN96QSLcAACANmiaTByul4FLJoebxYsXa9y4cUpKSlJmZqZuuOEG7d+//6LP2bRpk2w223m3ffv2dVHVAAB0X03DUuG69YJkcrjZvHmz7r33Xm3fvl0bNmyQ1+vVtGnTVFtbe8nn7t+/XyUlJcHboEGDuqBiAAC6t5qmOTdhPCxlamXr169vdv/5559XZmamdu7cqauvvvqiz83MzFRqamonVgcAAM7lYs5N21RVVUmS0tLSLnnuqFGjlJ2dralTp2rjxo0XPM/tdsvlcjW7AQCA9gn3rRekMAo3hmFo3rx5mjhxooYNG3bB87Kzs/X0009r7dq1evnll5Wfn6+pU6dqy5YtLZ6/ePFipaSkBG95eXmd1QQAACwv3LdekEwelvq8++67Tx988IHeeuuti56Xn5+v/Pz84P0JEyaouLhYS5YsaXEoa8GCBZo3b17wvsvlIuAAANBOXAreSvfff7/WrVunjRs3Kjc3t83PHz9+vA4cONDiY06nU8nJyc1uAACgfcJ9XynJ5J4bwzB0//3365VXXtGmTZvUv3//dr3Orl27lJ2dHeLqAADAuSJhzo2pld1777164YUX9Je//EVJSUkqLS2VJKWkpCguLk5SYFjp2LFjWrVqlSRp6dKl6tevnwoKCuTxeLR69WqtXbtWa9euNa0dAAB0F03hhjk3F7Bs2TJJ0uTJk5sdf/7553XXXXdJkkpKSlRUVBR8zOPxaP78+Tp27Jji4uJUUFCg1157TTNnzuyqsgEA6LYiYc6NzTAMw+wiupLL5VJKSoqqqqqYfwMAQBt94dd/1wmXW3+9f6KG9U7psvdty9/vsJhQDAAAIgPbLwAAAMvw+Q3VeXySwnv7BcINAABolaZ9paTwvlqKcAMAAFql2h2YTOx02BXjCN8IEb6VAQCAsBIJl4FLhBsAANBKZ8NN+A5JSYQbAADQSjXu8F/jRiLcAACAVoqErRckwg0AAGglhqUAAIClnO25YUIxAACwAObcAAAAS2FYCgAAWEoN4QYAAFiJi0X8AACAlTTNueFScAAAYAnMuQEAAJZS4ybcAAAAC2HjTAAAYCk1bL8AAACsoqHRJ4/PL4lhKQAAYAFNQ1I2m5QQQ7gBAAARrmkycWKMQ3a7zeRqLo5wAwAALik43ybMh6Qkwg0AAGiF6ghZwE9qZ7gpLi7W0aNHg/ffffddff/739fTTz8dssIAAED4sHzPzTe+8Q1t3LhRklRaWqovfelLevfdd7Vw4UL94he/CGmBAADAfME5N1btufnwww915ZVXSpLWrFmjYcOG6e2339YLL7ygFStWhLI+AAAQBiJldWKpneGmsbFRTqdTkvT3v/9d119/vSRp8ODBKikpCV11AAAgLFRHyAJ+UjvDTUFBgZYvX66tW7dqw4YNuu666yRJx48fV3p6ekgLBAAA5qs903OTYNVw8/DDD+upp57S5MmT9fWvf10jRoyQJK1bty44XAUAAKwjOCwVAeGmXRVOnjxZ5eXlcrlc6tGjR/D4f/7nfyo+Pj5kxQEAgPBg+aul6uvr5Xa7g8HmyJEjWrp0qfbv36/MzMyQFggAAMxXHbxaKrx3BJfaGW5mz56tVatWSZIqKyv1hS98Qb/97W91ww03aNmyZSEtEAAAmM/yPTfvvfeeJk2aJEn685//rKysLB05ckSrVq3S448/HtICAQCA+Wo9kTPnpl3hpq6uTklJSZKkN998UzfddJPsdrvGjx+vI0eOhLRAAABgvqaeG8teLTVw4EC9+uqrKi4u1htvvKFp06ZJksrKypScnBzSAgEAgPmqrb5C8YMPPqj58+erX79+uvLKKzVhwgRJgV6cUaNGhbRAAABgvqaem0hYobhdFX71q1/VxIkTVVJSElzjRpKmTp2qG2+8MWTFAQAA83l9ftU3+iRFRs9Nuyvs1auXevXqpaNHj8pms6l3794s4AcAgAXVun3Bry0758bv9+sXv/iFUlJS1LdvX/Xp00epqan65S9/Kb/fH+oaAQCAiWrOXCkV47ArxtGu6NCl2hW/Fi1apGeffVa/+c1v9MUvflGGYWjbtm362c9+poaGBj300EOhrhMAAJgkON8mAnptpHaGm5UrV+qZZ54J7gYuSSNGjFDv3r11zz33EG4AALCQGnejpMhYwE9q57BURUWFBg8efN7xwYMHq6KiosNFAQCA8FHdEDmXgUvtDDcjRozQE088cd7xJ554QsOHD+9wUQAAIHzURNAaN1I7h6UeeeQRffnLX9bf//53TZgwQTabTW+//baKi4v1+uuvh7pGAABgopru0HNzzTXX6JNPPtGNN96oyspKVVRU6KabbtJHH32k559/PtQ1AgAAEwV7biJkzk27q8zJyTlv4vD777+vlStX6rnnnutwYQAAIDxE2rBU+F+sDgAATBUcloqQnhvCDQAAuKimnptIWeeGcAMAAC6qaUfwSNh6QWrjnJubbrrpoo9XVla26c0XL16sl19+Wfv27VNcXJyuuuoqPfzww8rPz7/o8zZv3qx58+bpo48+Uk5Ojn7wgx9ozpw5bXpvAADQOrVWnnOTkpJy0Vvfvn11xx13tPr1Nm/erHvvvVfbt2/Xhg0b5PV6NW3aNNXW1l7wOYcOHdLMmTM1adIk7dq1SwsXLtTcuXO1du3atjQFAAC0UnD7hQiZc9OmKkN9mff69evPe/3MzEzt3LlTV199dYvPWb58ufr06aOlS5dKkoYMGaLCwkItWbJEN998c0jrAwAAn79aKtrkSlonrObcVFVVSZLS0tIueM4777yjadOmNTs2ffp0FRYWqrGx8bzz3W63XC5XsxsAAGi9aq6Wah/DMDRv3jxNnDhRw4YNu+B5paWlysrKanYsKytLXq9X5eXl552/ePHiZkNneXl5Ia8dAAArY52bdrrvvvv0wQcf6L//+78vea7NZmt23zCMFo9L0oIFC1RVVRW8FRcXh6ZgAAC6AcMwIi7chEWV999/v9atW6ctW7YoNzf3ouf26tVLpaWlzY6VlZXJ4XAoPT39vPOdTqecTmdI6wUAoLtwe/3y+QOdCAxLtYJhGLrvvvv08ssv65///Kf69+9/yedMmDBBGzZsaHbszTff1NixYxUdHRkTnQAAiBRN821sNik+OsrkalrH1HBz7733avXq1XrhhReUlJSk0tJSlZaWqr6+PnjOggULml1ePmfOHB05ckTz5s3T3r179dxzz+nZZ5/V/PnzzWgCAACWFhySinHIbj9/+kc4MjXcLFu2TFVVVZo8ebKys7ODt5deeil4TklJiYqKioL3+/fvr9dff12bNm3SyJEj9ctf/lKPP/44l4EDANAJIm1fKcnkOTdNE4EvZsWKFecdu+aaa/Tee+91QkUAAODzqt2BZVYiZesFKYyulgIAAOEn2HNDuAEAAFZQ64msrRckwg0AALgIem4AAIClVEfYAn4S4QYAAFxEU88NE4oBAIAlNK1zw5wbAABgCZG2r5REuAEAABcRiYv4EW4AAMAF0XMDAAAshXADAAAshXVuAACApQTXuWHODQAAsILapkvBndEmV9J6hBsAANAin99QnccniZ4bAABgAU2TiSUpwRllYiVtQ7gBAAAtago3MVF2OR2EGwAAEOEicQE/iXADAAAuoMbdKCmyLgOXCDcAAOACatxnJhMTbgAAgBUwLAUAACyFYSkAAGAp1RG49YJEuAEAABdQE4FbL0iEGwAAcAFnt14g3AAAAAsI9twQbgAAgBU0zblJINwAAAArYM4NAACwlKZ1bphzAwAALIGeGwAAYClMKAYAAJZCuAEAAJZhGAZ7SwEAAOtwe/3y+g1J9NwAAAALaFrjRpISYgg3AAAgwtV+br6N3W4zuZq2IdwAAIDzROpkYolwAwAAWnB264UokytpO8INAAA4z9kF/KJNrqTtCDcAAOA8Ne5GSZG39YJEuAEAAC0IrnFDuAEAAFZQ4/ZJirwF/CTCDQAAaEHTsBQ9NwAAwBIYlgIAAJZS7Y7MfaUkKfIqBgAAIVXn8erHr3yow6dq1dDoV4PXp5LKBkmR2XMTeRUDAICQ+t2GT/TyrmPnHbfZpCHZySZU1DGEGwAAurE9R6v07FuHJEk//vIQDcpKUqzDLmd0lLKSncpOiTO5wrYj3AAA0E15fX796OUP5DekWSNy9O1JA8wuKSSYUAwAQDf13LZD+ui4Sylx0XrwK0PNLidk6LkBQsDvN2Sc+dowAl/XuX1yNTTK1dCo6gav6jxeebyGvH6/Gn1+NXoNVdR5dLLaHbzVebxKio1WSly0kuMcSo6LVo/4GPWID/w3LSFGSbHRckTZFG23KyrKJofdJp/fkNdnqNHvl9dnyJChhBiHEpwOxcdEyemwyzCk+kafaj1e1bl9qm/0Berw+eXxGmr0+eUzDPn9hvyG5PMbstuk5LhAPYGaomUYhuo8PtW6varz+OQ3DGUlxyoj0Sm73WbmxwCgDYpO1en/bvhEkrRo5hBlJDlNrih0CDfA5zQ0+oJhpKbBq+oGr07VulVS1aDjlfU6XlmvkqoG1bi9qvcEAkK9xyev37j0i5so6kwA6kzRUTb1SolVdkqcoqNsgSsuGn1qaAx8f5wOu+Kio+SMjlJsdJQMw5DH65fH55e70S+/YSgtIUbpiU6lJ8QoPSFGCU6HYhx2xTjscjrsiomyK8pua3ardftUUetRRa1bp2o9qqxrVI07ECZr3T7VebzBz8cmyWazySYFg5zPMOTzGYqNjlJKfCDEpZ4JclF2m6JsZ98rwekIhsxArTHqnRqn2Ogw2DXZMKRTp6SaGikxUUpPD8wGBVpgGIYWvbpHDY1+jR+Qpq+NzTW7pJAyNdxs2bJFjz76qHbu3KmSkhK98soruuGGGy54/qZNmzRlypTzju/du1eDBw/uxEoRqfx+Q6WuBh0ur1VRRV3zUNLok6veq7LqBp1wNeiEy62q+saQvn9stF1JsdFKjg30okRH2eWw2xQdZVd0lE094mOUkeQM3uJjHKpxN6qqrlGuBq+q6ht1us6j07Uena4LfF3d4JXX5w/01py5RdlscpzpxYmOCow213q8amj0S1KzYGOzSQkxDsVGB3p0oqMCz3Gcqc1uk+x2m+y2QCByNTTKVd+oqvpGNfoCrxNltyk+JkrxMVGyyaaTNW41+gwVV9SruKI+pN/DSNArOVZ90uPVNy1e6YlOxUTZFOOwKzrKrtjoKPVOjVOf9Hjl9YhXXEyIg1BlpbRypfT//p/06adnj192mXT//dKdd0qpqaF9T0S8NYXF2nqgXDEOuxbfNFw2iwVhU8NNbW2tRowYoW9+85u6+eabW/28/fv3Kzn57KVpGRkZnVEeIoDH61fh4Qpt/+yUKuo8qnX7VOP2qtbtVXmNW0dO1cnt9bfpNW22wLoOybHRSop1KCUuWr1T45SdGquc1Dhlp8QqJS5asdFRiouOUnxMoHfBprP/ULbJpriYKMU4zJ3W5vMbqvUEvh/RUfYzocberl9khmGoodEvu12KiWr+Gl6fX2XV7kDvVlWDDMOQ0xGl2OjAH/foKJvcjX7VN/rU0OhXnccru812tkfGEXi907Uelde4VVHr0akaj2o93ma9O54zoc5vGPL5A7e4mCilB3tSnOoRH61EZ7QSnIHPJj4mStFR9uBwoWFIhoxgj4z9TO9MQ6NPlfWBYFlZfyZE+s/07pzp4al1e1VR26iKWrdO1zWqzNWgWo9Ppa4Glboa9O6hikt+HzOSnOrfM0GDeyUpv1eS8rOSdHmvJCXHRrf5M9Ebb0g33yzV1Z3/2GefSQ88IC1aJK1dK02f3vbXh+UYhqGntnymh9fvkyR9b+og9e+ZYHJVoWdquJkxY4ZmzJjR5udlZmYqlX+JdEs1bq+KK+q0q6hSG/eX6e2D5ar1+C76HIfdpry0ePVNj1dqXLTiYqKCwSQx1qGspFhlJccqK9mpzKRYJcU6LDN3JMpuU3JsdPv+cJ7DZrNdsNfBEWVXTmqcclIj75LRjjAMQxW1Hh2pqFPRqTodOVUnV0PjmXlMgTBW5/bpaGXgseoGb3B+1blBKC8tTgXZKSrISdbQnGRdkZuizKTYC7/5G29IX/7ymbTWwpBj07H6+sB5r71GwOnmGhp9WvjKHr38XmA9m/8Y30ffvdoaV0edKyLn3IwaNUoNDQ0aOnSofvzjH7c4VNXE7XbL7XYH77tcrq4oER1gGIaOnq7XvtJq7Stxad+JahVX1Km4ok6n684fNuqZGKOrB2UoNy1eic4oJTgdSnQ6lBofo37p8eqdGidHFBcGIvRsNltgjlCiU6P79Ljk+VV1jTpSUauDZTXaX1qt/Seqtb+0WiVVDcEhvfUflQbPz0uL0+g+PTSmbw+N7tND+b2SAsOOlZWBHhvDkPyX6Jn0+yW7PXD+0aMMUXVTZdUNmvPHnXqvqFJRdpt+Nmuobp/Qz+yyOk1EhZvs7Gw9/fTTGjNmjNxut/74xz9q6tSp2rRpk66++uoWn7N48WL9/Oc/7+JK0RYNjT7tKqrUvw6d0r8+q9CeY1WqObOnSUt6xEdrYGairh6UoSmDMzU0O9kyPS2wtpT4aA2PT9Xw3NRmx6vqGvVRSZU+Pu7SR8dd+uh4lQ6U1QQDz192H5cUmMM1LCdF337vfzW9rk62lnpsWuL3B4auVq2S5s4NcasQ7jbuK9PCV/aopKpBybEO/f62MZo4qKfZZXUqm2G09v+OzmWz2S45obgls2bNks1m07p161p8vKWem7y8PFVVVTWbt4Ou4/Mb2nOsSls/OamtB8u1u6hSHl/zf31GR9k0MDNJQ7KTNLhXkvqlJygvLV65PeKUFIIhFiDcVTc0andxpd47UqmdRae1q+i0qhu8kmFo09PfUZ/K0rYtVGazSQMGSAcOcBVVN1FSVa9f/O/H+tuHgd7AARkJevbOcRE7x8blciklJaVVf78jquemJePHj9fq1asv+LjT6ZTTaZ1r9yOFYRgqq3arzOVWea1bp2o8OlXj1p5jVdp2sPy84aXMJKe+MCBdX+ifprH9euiyjMTgVT9Ad5QUG61JgzI0aVDgggm/39ChU7Xa+8Fn6vdI6SWe3QLDCFxNVVERuEwcluX1+bXynSP6v2/uV63Hpyi7Td+a2F/fmzpICRG4CWZ7RHwrd+3apezsbLPL6NYMw9Cxynp9eKxKe45V6YOjVfrwWFWL82OaJDkdumpguq6+PENXXdZT/dLjLXcpIhBKdrtNl2Uk6rLLOtbjvHrDh8odNUTDeqeoZyL/8LOaolN1mvviLu0urpQkje6TqoduvCIiN7/sCFPDTU1NjQ4ePBi8f+jQIe3evVtpaWnq06ePFixYoGPHjmnVqlWSpKVLl6pfv34qKCiQx+PR6tWrtXbtWq1du9asJnQbPr+hqvpGVdR6dLrOozKXWx8dD4SZCwWZKLtNPRNjlJ7gVHpijHomOpWXFq9Jg3pqZF4qPTNAeyQmdujpS945rsrdNZIC6/MM6x24QqsgJ1kFvVOUkxLLPzQi1Ku7junHr36oGrdXSbEOLZgxRP8+Lq9bzkk0NdwUFhY2u9Jp3rx5kqQ777xTK1asUElJiYqKioKPezwezZ8/X8eOHVNcXJwKCgr02muvaebMmV1eu9X5/YbeP1qpNz8+oQ0fn9CnJ2tavNq0icNu0+VZSRqem6JhvVN0Re8U5fdKCo+VWwErSU8PLND32WctXwJ+AYbNpqrsPF39hXx9WOLSofLa4Po8f997Inhej/hoDc1JDs5zy+sRmOvWu0ec0uJjuuUfynBX4/bqwb98GLzEe2zfHlr67yOV2yPe5MrMEzYTirtKWyYkdUd7S1xavf2INnx8QmXV7vMeT451KC0hRj0SYpSflUSQAczw2GOBBfra8uvbZpOWLg1eLVXj9mpviUt7jlY1u0LrYtt0xETZlZnsVK/kWPVKiVWftHj165mgfukJ6tczXhmJTnp9OolhGHr3UIVWbT+inYdPS5LstsDFODXuwGrmdps0d+og3TdloCWXv2jL32/CDSRJO4+c1u83HtQ/9pUFjyU6HZoyOFNfGpql8f3T1CMhhqEkIBxUVkq5uYEF+i61zo0UWOcmLu6S69w0NPr0yYlq7SupVlFFnYpP1+no6XoVV9S1+I+dczkddvVMDGwlcva/gSHpnolnh6d7JsYoJS6aINQKdR6vXt11XKveOax9pdUXPK93apyW/vtIjeuX1oXVda1udbUU2q/R59eWT07qD1s/0/bPAqul2mzSzCuydcvYPI0fkCang94YIOykpga2VPjylwPB5WIBx24P/I/98suXXMAvNjpKw3PPX4dHCmx10rQPW2mVWyVV9Tpyqk6HT9XqUHmtjlXWy+3161hlvY5VXnp/MYfdFgw7AzISNSQ7SUN6JWtwdpJ6JTPvR5L+ue+E/mvN+8E5jbHRdt04qrduHJWr+JiowBqOZ/on6D1vjp6bbsbvN/Re0Wm9uvuYXvugJPg/TXSUTTeNytV3rxmgARkdm7AIoIucu7fU53+dN4WD+PhAsJk2rVNLcXt9KnO5VVbtVnlN4HayOrAMRHnN2f+erHEH1uu5iKRYhwZlJmpQZpIGZiZqYFaiRuSmKi0hplPbEC78fkNPbDyo3/39ExmG1Dc9XreP76uvjclTSnz3XeeLYamL6K7hprzGrRffLdJ/v1vc7F9VPRNjdMPI3rp7Yv9uty8QYAmVlYGVhx9//PxdwefODewKnpJiWnktcXt9qqj1qLzaoxOuBh0oq9G+Upf2lVTr05M18l5g3s+gzESN65+mK/ulaXhuirKSY8Ny3Rb/mQ1rA5vGtm0o39XQqHkvvR+c5H37+L76yVeGmr4Jbzgg3FxEdws37xdXauU7h/XX90uCqwAnOh2aXtBLs0fm6KrL0i058QzodgwjsEBfdbWUlCSlpUXkSsRur0+Hy+t0oKxaB07U6OCZ4PPpydoWz4+PiQrO8Ul0Bna9j4sObI4bH+NQcpwjsHlsXLQSYqJ0vKpBB8tq9OnJGn1aVqNqt1d5PeLUJy1efdLilZcWr8syE3V5VlKrLovfW+LSn/51RIWHAytIuxoaVeP2BjvRkpwO9UiIUY/4aKXGxygx1qHk2MD+d4nOaMU47IqyS/Yzu9T/cfsRfXayVjEOu351wzDdMjYv1N/iiEW4uYjuEm4OllXrx69+GJxLI0kjclN051X9NPOKbMZmAUSUilqPdhyu0I5DFdpxuEIHympU5/F16nsmOR0amJWoQZmJuiwjUQMyEnVZRoKykmP15selWr29SDuPnA75+2anxGr5f4zRiLzUkL92JCPcXITVw43b69OyTZ/q9xs/lcfnV3SUTV++Ilt3XtVPo1qxazEARIpatzc4t+dktVt1Hp/qG31qaPSp3uNTjccb6E2pb5SrwauahkZlJcfqsoxEDTwTWJLjHDp6ul5FFXUqqqjTkVOBXds/O1l7weGxz3PYbZpWkKXZI3srKzlWSbGBnqKkWIfqPT6drgssfFpR26jKOo9q3F7VNHhV4/bK1eCV1+eXz2/IZxjy+Q31THTqvmsHsnp0C7haqpvaeaRCP1y7RwfLAquPXjs4U7+YXdCtF3ICYF0JTocSnA71Te/YRpB90xP0xXOOebx+HT5Vq09OBIbHPiuv1WcnA6GnvtGn7JRYff3KPvr3cXnKTI5t8XVjo6PUo5tMgg43hBsLMAxDT248qN9uCMys75kYo5/OKtBXhmdzOSUAtEOMw67Ls5J0eVZSs+N+v6FTtR6lJcQoitWawxbhJsL5/YZ+/r8faeU7RyRJXxuTq0VfHqLUeP61AAChZrfblJHEkFG4I9xEMI/Xr//6n/f1v+8flyT9dNZQffOL/U2uCgAAcxFuIlSt26s5q3dq64FyOew2/faWEZo9srfZZQEAYDrCTQQqczXo26sK9cHRKsXHRGnZf4zRNZdnmF0WAABhgXATYfYcrdJ3VhWq1NWgHvHRev6bV2okayEAABBEuIkgf/3guOb/z/tqaPRrYGainr1zbIcvgQQAwGoINxHA7zf02D8O6LF/HJAkTc7P0ONfH6Xk2O67gRoAABdCuIkAj7yxX8s3BzbE+/bE/lowcwjrKwAAcAGEmzD3j70ngsHmoRuH6bYv9DW5IgAAwhvbQYexo6frNG/N+5Kku67qR7ABAKAVCDdhyuP1674XdqmqvlEjclO0cOYQs0sCACAiEG7C1MPr92l3caWSYx164hujFePgowIAoDX4ixmG3vioVM++dUiS9OjXRigvjV29AQBoLcJNmKluaNQP/vyBJOlbE/trekEvkysCACCyEG7CzJrCo6qqb9RlGQn64XWDzS4HAICIQ7gJIz6/oZVvH5Yk3T2xP/NsAABoB/56hpF/7D2hooo6pcRF66ZRuWaXAwBARCLchJHntx2WJH39yj6Ki4kytxgAACIU4SZM7C1x6Z3PTinKbtMdE1isDwCA9iLchInntwUu/b5uWC/lpMaZXA0AAJGLcBMGTtW49eru45Kku7/Y3+RqAACIbISbMPDCv4rk8fo1IjdFo/ukml0OAAARjXBjMo/Xr1Xbj0gKXP5ts9lMrggAgMhGuDHZ63tKdLLarcwkp2YMyza7HAAAIh7hxmR//aBEUuDybxbtAwCg4/hraiKP16+3Py2XJH1paJbJ1QAAYA2EGxMVHqlQncennolODc1ONrscAAAsgXBjos2fnJQkXX15T9ntTCQGACAUCDcm2rw/EG4m52eaXAkAANZBuDHJCVeD9pVWy2aTJg3saXY5AABYBuHGJE29NiNyU9UjIcbkagAAsA7CjUma5ttcc3mGyZUAAGAthBsTeH1+bT1wJtzkE24AAAglwo0J3j9aKVeDVylx0RqRm2p2OQAAWArhxgRN820mDeqpKC4BBwAgpAg3JmC+DQAAnYdw08VO1bj1wbEqSYQbAAA6A+Gmi711sFyGIQ3JTlZmcqzZ5QAAYDmEmy7WNN+GXhsAADoH4aYL+f2Gthwg3AAA0JkIN13ow+NVKq/xKCEmSmP69jC7HAAALIlw04U27mu6BDxDMQ6+9QAAdAZT/8Ju2bJFs2bNUk5Ojmw2m1599dVLPmfz5s0aM2aMYmNjNWDAAC1fvrzzCw2RjfvLJElTBjMkBQBAZzE13NTW1mrEiBF64oknWnX+oUOHNHPmTE2aNEm7du3SwoULNXfuXK1du7aTK+24UzVuvX+0UpI0OT/T3GIAALAwh5lvPmPGDM2YMaPV5y9fvlx9+vTR0qVLJUlDhgxRYWGhlixZoptvvrmTqgyNzZ+clGFIQ7OTlcUl4AAAdJqImvjxzjvvaNq0ac2OTZ8+XYWFhWpsbGzxOW63Wy6Xq9nNDBvPXALOkBQAAJ0rosJNaWmpsrKymh3LysqS1+tVeXl5i89ZvHixUlJSgre8vLyuKLUZr8+vLWe2XLh2MENSAAB0pogKN5JkszXfaNIwjBaPN1mwYIGqqqqCt+Li4k6v8Vy7iitVVd+o1PhojczjEnAAADqTqXNu2qpXr14qLS1tdqysrEwOh0Pp6ektPsfpdMrpdHZFeRe0cV/gKqmrB2WwCzgAAJ0sonpuJkyYoA0bNjQ79uabb2rs2LGKjo42qapLY74NAABdx9RwU1NTo927d2v37t2SApd67969W0VFRZICQ0p33HFH8Pw5c+boyJEjmjdvnvbu3avnnntOzz77rObPn29G+a1SWtWgvSUu2WyBnhsAANC5TB2WKiws1JQpU4L3582bJ0m68847tWLFCpWUlASDjiT1799fr7/+uh544AE9+eSTysnJ0eOPPx7Wl4E3Ldw3Mi9V6YnmDo8BANAdmBpuJk+eHJwQ3JIVK1acd+yaa67Re++914lVhVbTfJspLNwHAECXiKg5N5HG7fVp28HAJeqEGwAAugbhphMVHj6tWo9PPROdKshJNrscAAC6BcJNJ2oakpqcnyE7l4ADANAlCDed6J9nJhOzKjEAAF2HcNNJjpyq1WcnaxVlt2nioJ5mlwMAQLdBuOkkm84s3De2bw8lx4bvAoMAAFgN4aaTNK1vM4UhKQAAuhThphPUe3x659NTkphvAwBAVyPcdIJ3PiuX2+tX79Q4DcpMNLscAAC6FcJNJ9i4LzDfZnJ+hmw2LgEHAKArEW5CzDCMs/NtWJUYAIAuR7gJsYNlNTp6ul4xDruuGphudjkAAHQ7hJsQa+q1GT8gXfExpu5LCgBAt0S4CbGm+TZT8jNMrgQAgO6JcBNC1Q2N2nG4QhLzbQAAMAvhJoS2HSyX12+of88E9euZYHY5AAB0S4SbEPrn53YBBwAA5iDchEjgEvDAfBtWJQYAwDyEmxD56LhLJ6vdiouO0pX908wuBwCAbotrlUNkYGaiVt19pY5V1svpiDK7HAAAui3CTYjERkfp6suZawMAgNkYlgIAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJbS7XYFNwxDkuRyuUyuBAAAtFbT3+2mv+MX0+3CTXV1tSQpLy/P5EoAAEBbVVdXKyUl5aLn2IzWRCAL8fv9On78uK699loVFhY2e2zcuHHasWNHq+639LXL5VJeXp6Ki4uVnJzc7hrPfd/2nNfSY605drE2/uMf/wjr9rV0vLt9hhdqr1Xad+59fkbDr43t/Rlt+trqn6HV29eZP6OGYai6ulo5OTmy2y8+q6bb9dzY7Xbl5ubK4XCc902Piopqduxi9y/0tSQlJyd36AM99/Xac15Lj7XmWGvaGK7ta+l4d/sML9Req7Tv3Pv8jIZfG9v7M3ru1+HavpaO8zPadT+jl+qxadJtJxTfe++9lzx2sfsX+rqzamvrea1pX0vHuqKNndW+lo53t8/wQu21SvvOvc/PaPuF289oW2pqDat/hlZvX0dfr9sNS3Uml8ullJQUVVVVdSithiurt0+yfhtpX+SzehtpX+QLhzZ2256bzuB0OvXTn/5UTqfT7FI6hdXbJ1m/jbQv8lm9jbQv8oVDG+m5AQAAlkLPDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCjUl+97vfqaCgQEOHDtXcuXNbtRFYJNm/f79GjhwZvMXFxenVV181u6yQOnTokKZMmaKhQ4fqiiuuUG1trdklhZTD4Qh+ft/+9rfNLqfT1NXVqW/fvpo/f77ZpYRUdXW1xo0bp5EjR+qKK67QH/7wB7NLCrni4mJNnjxZQ4cO1fDhw/U///M/ZpcUcjfeeKN69Oihr371q2aXEhJ//etflZ+fr0GDBumZZ57ptPfhUnATnDx5UuPHj9dHH32k6OhoXX311VqyZIkmTJhgdmmdoqamRv369dORI0eUkJBgdjkhc8011+hXv/qVJk2apIqKCiUnJ8vhsM6OJj179lR5ebnZZXS6RYsW6cCBA+rTp4+WLFlidjkh4/P55Ha7FR8fr7q6Og0bNkw7duxQenq62aWFTElJiU6cOKGRI0eqrKxMo0eP1v79+y31e2bjxo2qqanRypUr9ec//9nscjrE6/Vq6NCh2rhxo5KTkzV69Gj961//UlpaWsjfi54bk3i9XjU0NKixsVGNjY3KzMw0u6ROs27dOk2dOtVSv3CagumkSZMkSWlpaZYKNt3FgQMHtG/fPs2cOdPsUkIuKipK8fHxkqSGhgb5fD7L9RBnZ2dr5MiRkqTMzEylpaWpoqLC3KJCbMqUKUpKSjK7jJB49913VVBQoN69eyspKUkzZ87UG2+80SnvRbhpwZYtWzRr1izl5OTIZrO1OJzy+9//Xv3791dsbKzGjBmjrVu3tvr1MzIyNH/+fPXp00c5OTn6t3/7N1122WUhbMGldXYbP2/NmjW69dZbO1hx23R2+w4cOKDExERdf/31Gj16tH7961+HsPpL64rPz+VyacyYMZo4caI2b94cospbryvaOH/+fC1evDhEFbdNV7SvsrJSI0aMUG5urn7wgx+oZ8+eIaq+dbry90xhYaH8fr/y8vI6WHXrdWX7wkFH23v8+HH17t07eD83N1fHjh3rlFoJNy2ora3ViBEj9MQTT7T4+EsvvaTvf//7WrRokXbt2qVJkyZpxowZKioqCp4zZswYDRs27Lzb8ePHdfr0af31r3/V4cOHdezYMb399tvasmVLVzVPUue3sYnL5dK2bdu6/F/Gnd2+xsZGbd26VU8++aTeeecdbdiwQRs2bOiq5nXJ53f48GHt3LlTy5cv1x133CGXy9UlbWvS2W38y1/+ossvv1yXX355VzWpma74DFNTU/X+++/r0KFDeuGFF3TixIkuaVuTrvo9c+rUKd1xxx16+umnO71Nn9dV7QsXHW1vSz2HNputc4o1cFGSjFdeeaXZsSuvvNKYM2dOs2ODBw82fvSjH7XqNdesWWPcc889wfuPPPKI8fDDD3e41vbqjDY2WbVqlXHbbbd1tMQO6Yz2vf3228b06dOD9x955BHjkUce6XCt7dGZn1+T6667ztixY0d7S+ywzmjjj370IyM3N9fo27evkZ6ebiQnJxs///nPQ1Vym3TFZzhnzhxjzZo17S2xwzqrjQ0NDcakSZOMVatWhaLMduvMz3Djxo3GzTff3NESQ6o97d22bZtxww03BB+bO3eu8ac//alT6qPnpo08Ho927typadOmNTs+bdo0vf322616jby8PL399tvBcfBNmzYpPz+/M8ptl1C0sYkZQ1KXEor2jRs3TidOnNDp06fl9/u1ZcsWDRkypDPKbbNQtO/06dNyu92SpKNHj+rjjz/WgAEDQl5re4WijYsXL1ZxcbEOHz6sJUuW6Dvf+Y4efPDBzii3zULRvhMnTgR721wul7Zs2WK53zOGYeiuu+7Stddeq9tvv70zymy3UP4ejQStae+VV16pDz/8UMeOHVN1dbVef/11TZ8+vVPqYQZkG5WXl8vn8ykrK6vZ8aysLJWWlrbqNcaPH6+ZM2dq1KhRstvtmjp1qq6//vrOKLddQtFGSaqqqtK7776rtWvXhrrEDglF+xwOh37961/r6quvlmEYmjZtmr7yla90RrltFor27d27V9/97ndlt9tls9n02GOPdcoVDe0Vqp/RcBWK9h09elTf+ta3ZBiGDMPQfffdp+HDh3dGue0SijZu27ZNL730koYPHx6c//HHP/5RV1xxRajLbbNQ/YxOnz5d7733nmpra5Wbm6tXXnlF48aNC3W5Hdaa9jocDv32t7/VlClT5Pf79YMf/KDTrt4j3LTTueOEhmG0aezwoYce0kMPPRTqskKqo21MSUnp8jH+tuho+2bMmKEZM2aEuqyQ6Uj7rrrqKu3Zs6czygqpjn6GTe66664QVRRaHWnfmDFjtHv37k6oKrQ60saJEyfK7/d3Rlkh09Gf0c66mqizXKq9119/fZf8Y55hqTbq2bOnoqKizkveZWVl5yXWSGX1NtK+yGf1Nlq9fZL122j19p0r3NpLuGmjmJgYjRkz5rwrYzZs2KCrrrrKpKpCy+ptpH2Rz+pttHr7JOu30ertO1e4tZdhqRbU1NTo4MGDwfuHDh3S7t27lZaWpj59+mjevHm6/fbbNXbsWE2YMEFPP/20ioqKNGfOHBOrbhurt5H2RXb7JOu30ertk6zfRqu371wR1d5OuQYrwm3cuNGQdN7tzjvvDJ7z5JNPGn379jViYmKM0aNHG5s3bzav4HawehtpX2S3zzCs30art88wrN9Gq7fvXJHUXvaWAgAAlsKcGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwARqV+/flq6dKnZZQAIQ6xQDOCC7rrrLlVWVurVV181u5TznDx5UgkJCYqPjze7lBaF8/cOsDp6bgCElcbGxladl5GRYUqwaW19AMxDuAHQbh9//LFmzpypxMREZWVl6fbbb1d5eXnw8fXr12vixIlKTU1Venq6vvKVr+jTTz8NPn748GHZbDatWbNGkydPVmxsrFavXq277rpLN9xwg5YsWaLs7Gylp6fr3nvvbRYszh2WstlseuaZZ3TjjTcqPj5egwYN0rp165rVu27dOg0aNEhxcXGaMmWKVq5cKZvNpsrKygu20Wazafny5Zo9e7YSEhL0q1/9Sj6fT9/61rfUv39/xcXFKT8/X4899ljwOT/72c+0cuVK/eUvf5HNZpPNZtOmTZskSceOHdOtt96qHj16KD09XbNnz9bhw4fb9wEAaBHhBkC7lJSU6JprrtHIkSNVWFio9evX68SJE7rllluC59TW1mrevHnasWOH/vGPf8hut+vGG2+U3+9v9lo//OEPNXfuXO3du1fTp0+XJG3cuFGffvqpNm7cqJUrV2rFihVasWLFRWv6+c9/rltuuUUffPCBZs6cqdtuu00VFRWSAkHqq1/9qm644Qbt3r1b3/3ud7Vo0aJWtfWnP/2pZs+erT179ujuu++W3+9Xbm6u1qxZo48//lgPPvigFi5cqDVr1kiS5s+fr1tuuUXXXXedSkpKVFJSoquuukp1dXWaMmWKEhMTtWXLFr311ltKTEzUddddJ4/H09pvPYBLMWUvcgAR4c477zRmz57d4mM/+clPjGnTpjU7VlxcbEgy9u/f3+JzysrKDEnGnj17DMMwjEOHDhmSjKVLl573vn379jW8Xm/w2Ne+9jXj1ltvDd7v27ev8bvf/S54X5Lx4x//OHi/pqbGsNlsxt/+9jfDMAzjhz/8oTFs2LBm77No0SJDknH69OmWvwFnXvf73//+BR9vcs899xg333xzszac+7179tlnjfz8fMPv9wePud1uIy4uznjjjTcu+R4AWoeeGwDtsnPnTm3cuFGJiYnB2+DBgyUpOPT06aef6hvf+IYGDBig5ORk9e/fX5JUVFTU7LXGjh173usXFBQoKioqeD87O1tlZWUXrWn48OHBrxMSEpSUlBR8zv79+zVu3Lhm51955ZWtamtL9S1fvlxjx45VRkaGEhMT9Yc//OG8dp1r586dOnjwoJKSkoLfs7S0NDU0NDQbrgPQMQ6zCwAQmfx+v2bNmqWHH374vMeys7MlSbNmzVJeXp7+8Ic/KCcnR36/X8OGDTtvCCYhIeG814iOjm5232aznTec1ZbnGIYhm83W7HGjlReLnlvfmjVr9MADD+i3v/2tJkyYoKSkJD366KP617/+ddHX8fv9GjNmjP70pz+d91hGRkaragFwaYQbAO0yevRorV27Vv369ZPDcf6vklOnTmnv3r166qmnNGnSJEnSW2+91dVlBg0ePFivv/56s2OFhYXteq2tW7fqqquu0j333BM8dm7PS0xMjHw+X7Njo0eP1ksvvaTMzEwlJye3670BXBrDUgAuqqqqSrt37252Kyoq0r333quKigp9/etf17vvvqvPPvtMb775pu6++275fL7g1UBPP/20Dh48qH/+85+aN2+eae347ne/q3379umHP/yhPvnkE61ZsyY4QfncHp1LGThwoAoLC/XGG2/ok08+0U9+8hPt2LGj2Tn9+vXTBx98oP3796u8vFyNjY267bbb1LNnT82ePVtbt27VoUOHtHnzZn3ve9/T0aNHQ9VUoNsj3AC4qE2bNmnUqFHNbg8++KBycnK0bds2+Xw+TZ8+XcOGDdP3vvc9paSkyG63y26368UXX9TOnTs1bNgwPfDAA3r00UdNa0f//v315z//WS+//LKGDx+uZcuWBa+WcjqdbXqtOXPm6KabbtKtt96qL3zhCzp16lSzXhxJ+s53vqP8/PzgvJxt27YpPj5eW7ZsUZ8+fXTTTTdpyJAhuvvuu1VfX09PDhBCrFAMoNt66KGHtHz5chUXF5tdCoAQYs4NgG7j97//vcaNG6f09HRt27ZNjz76qO677z6zywIQYoQbAN3GgQMH9Ktf/UoVFRXq06eP/uu//ksLFiwwuywAIcawFAAAsBQmFAMAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEv5/+C1DXtsiFoHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LitModel(max_lr)\n",
    "print('Parameter count: ', parameter_count(model))\n",
    "\n",
    "# Create Sequence DataModule and specifically train DataLoader\n",
    "data = SeqDataModule(train_path=train_path,\n",
    "                     valid_path=valid_path,\n",
    "                     test_path=test_path)\n",
    "\n",
    "train_dl = data.train_dataloader()\n",
    "\n",
    "# Create Trainer, pass selected gpu device\n",
    "dump_dir = model_dir / \"model\"\n",
    "trainer = pl.Trainer(accelerator='gpu',\n",
    "                     devices=[device], \n",
    "                     precision='16-mixed',\n",
    "                     gradient_clip_val=1,\n",
    "                     default_root_dir=dump_dir)\n",
    "\n",
    "# Create pl.tuner.Tuner instance\n",
    "tuner = pl.tuner.Tuner(trainer)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = tuner.lr_find(model,\n",
    "                          train_dataloaders=train_dl,\n",
    "                          attr_name='max_lr')\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "print(f'Suggested lr: {new_lr:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Here we'll train LegNet model with optimal learning rate (which was found on previous step by LR Finder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  1323169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type            | Params\n",
      "------------------------------------------------\n",
      "0 | model       | LegNet          | 1.3 M \n",
      "1 | loss        | MSELoss         | 0     \n",
      "2 | val_pearson | PearsonCorrCoef | 0     \n",
      "------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.293     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ffe5ebbbfe4e68bd515c65f0ba3c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba633c6ad3b47898f01e693dde2cd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikgr/miniconda3/envs/legnet/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f875967b83d4a67b830f75cfabb4654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb6f18987542f49fdd912dd1261a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acb5c768a4543c682ed21867efc9fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22fb3991c9647fab7a8001310f9f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2d01d1012146b1998207a2e7e96ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d1a60e6364556abd439d24365a750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c194b460e9eb4e8f94b9aab07e900ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9117961056e841969633a7eaafa897d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acea7c4240a844f18c16f9a58760dba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3347e6b65dbc4ec1a8f88fd736ac0450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be10630de8a04d2ba5394612b4282e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a317ce0975184359a0482c8488649c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3818b74da64de38b4e1530f24414f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5be47309d94322b2911c71e2039066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6902134f934a76b52ff7e099c845ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6e8415f0c4ec89c03e6a5c29544ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a66eb6e78b4d99b24b4c9b1a22364c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c861876ace84ecebb237c90f15cd2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773952883efc4016ac193d4b06e654e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4895d4eb5d924dcd9623d997614fe416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LitModel.__init__() missing 1 required positional argument: 'max_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m                      enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m                      devices\u001b[38;5;241m=\u001b[39m[device], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m                      gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     30\u001b[0m                      default_root_dir\u001b[38;5;241m=\u001b[39mdump_dir)\n\u001b[1;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, \n\u001b[1;32m     33\u001b[0m     datamodule\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m LitModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1520\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1448\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1522\u001b[0m         checkpoint_path,\n\u001b[1;32m   1523\u001b[0m         map_location,\n\u001b[1;32m   1524\u001b[0m         hparams_file,\n\u001b[1;32m   1525\u001b[0m         strict,\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1527\u001b[0m     )\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:89\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 89\u001b[0m     storage \u001b[38;5;241m=\u001b[39m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, strict\u001b[38;5;241m=\u001b[39mstrict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/legnet/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:141\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 141\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_cls_kwargs)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: LitModel.__init__() missing 1 required positional argument: 'max_lr'"
     ]
    }
   ],
   "source": [
    "model = LitModel(new_lr)\n",
    "print(\"Model parameters: \", parameter_count(model).item())\n",
    "\n",
    "# Create train & test DataLoaders\n",
    "train_dl = data.train_dataloader()\n",
    "valid_dl = data.val_dataloader()\n",
    "\n",
    "# Setup model checkpoints for the model from the last epoch and for the best (in terms of pearson correlation on validation split) model\n",
    "dump_dir = model_dir / f\"model\"\n",
    "last_checkpoint_callback = pl.callbacks.ModelCheckpoint(   #type: ignore\n",
    "    save_top_k=1,\n",
    "    monitor=\"step\",\n",
    "    mode=\"max\",\n",
    "    filename=\"last_model-{epoch}\",\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "        \n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_pearson\",\n",
    "    mode=\"max\",\n",
    "    filename=\"pearson-{epoch:02d}-{val_pearson:.2f}\",\n",
    ")\n",
    "\n",
    "# Create Trainer, pass callbacks and desired number of epochs\n",
    "trainer = pl.Trainer(accelerator='gpu',\n",
    "                     enable_checkpointing=True,\n",
    "                     devices=[device], \n",
    "                     precision='16-mixed', \n",
    "                     max_epochs=epoch_num,\n",
    "                     callbacks=[last_checkpoint_callback,  best_checkpoint_callback],\n",
    "                     gradient_clip_val=1,\n",
    "                     default_root_dir=dump_dir)\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model, \n",
    "    datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Here is code for predictions generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42703d6264448bf8d18f658bf0e9196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2813edfd1cc147c59c1063a2eb50c3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best model and test it (create prediction for test data)\n",
    "model = LitModel.load_from_checkpoint(best_checkpoint_callback.best_model_path, max_lr=new_lr)\n",
    "df_pred = save_predict(trainer, \n",
    "                       model, \n",
    "                       data,\n",
    "                       save_dir=dump_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
